{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning: prediction\n",
    "\n",
    "Machine learning-trained algorithms can predict whether bioacoustic recordings contain a sound of interest. For instance, an algorithm trained how to detect the sound of a Wood Thrush can be used to predict where Wood Thrushes vocalize in a set of autonomous recordings. \n",
    "\n",
    "Note: \n",
    "- The Kitzes Lab, the developers of OpenSoundscape, pre-trained a series of [baseline machine learning models](https://pitt.box.com/s/a6jeamnew098vp5a9a7m1h9j5rce6t6y) that can be used to predict the presence of [506 species of common North American birds](https://pitt.app.box.com/s/d0snd1tyilscksbxc36q2slz6s4aa2ag). These are our \"beta\" models and are for demonstration purposes only, not for research use. We hope to make our more accurate models available soon. \n",
    "- If we are to use these machine learning models for research, contact the [Kitzes Lab](https://kitzeslab.org).\n",
    "- There are example models provided by the Kitzes lab here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resample the data\n",
    "We want to resample the data to 44.1kHz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules\n",
    "Import the following modules to run a pre-trained machine learning learning classifier. First, from OpenSoundscape we will need two classes (`Audio` and `SingleTargetAudioDataset`) and three functions (`run_command`, `lowercase_annotations`, and `predict`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opensoundscape.audio import Audio\n",
    "from opensoundscape.datasets import SingleTargetAudioDataset\n",
    "from opensoundscape.helpers import run_command\n",
    "from opensoundscape.raven import lowercase_annotations\n",
    "from opensoundscape.torch.predict import predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the following machine learning-related modules. OpenSoundscape uses PyTorch to do machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn\n",
    "import torchvision.models\n",
    "import torch.utils.data\n",
    "import torchvision.transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, use a few miscellaneous functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import os.path\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from math import floor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download model\n",
    "\n",
    "To use the model, it must be downloaded onto your computer and loaded with the same specifications it was created with.\n",
    "\n",
    "Download the example model for Wood Thrush, *Hylocichla mustelina*. First, create a folder called `\"prediction_example\"` to store the model and its data in.\n",
    "\n",
    "To do: \n",
    "- Access the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name = \"prediction_example\"\n",
    "folder_path = Path(folder_name)\n",
    "if not folder_path.exists(): folder_path.mkdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model\n",
    "At its core, a machine learning model consists of two things: its architecture and its weights. \n",
    "\n",
    "The architecture is the complex structure of the model, which in this case, is a convolutional neural network. Convolutional neural networks are a particular set of algorithms especially suited to extracting and interpreting features from images, such as combinations of lines, dots, and edges. In this case, we use a `resnet18` convolutional neural network. After feature extraction, the convolutional neural network's features are passed to a classifier. The classifier decides how to weight each feature in predicting the final class identity. The model was trained with a `Linear` classifier.\n",
    "\n",
    "Create the architecture of the model. First, designate the model as a `resnet18` CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.resnet18(pretrained=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, add the `fc` layers. \"FC\" stands for \"fully connected\". To set up the proper architecture, we need to specify the correct number of input features, output features, and classifier type. \n",
    "\n",
    "The number of input features to the FC is equal to the number of features extracted from the convolutional neural network and passed to the the FC layer: `model.fc.in_features`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cnn_features = model.fc.in_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models were trained to predict two classes (bird present and bird absent), so the number of output features of the FC layer is 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the classifier type is a `torch.nn.Linear` classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc = torch.nn.Linear(\n",
    "    in_features = num_cnn_features,\n",
    "    out_features = num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weights of the model are distinguished from its architecture because, while the architecture is decided by humans, the weights of the architecture are learned during the machine learning process. When downloading the machine learning model, you downloaded the weights. \n",
    "\n",
    "First, use `torch.load` to get the model weights from the filename."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch9 = torch.load('/home/e4e/Desktop/model_train_results/epoch-9.tar')\n",
    "weights = epoch9['model_state_dict'] # the last epoch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the key of the weights of the epoch compared to the example weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(weights) <class 'collections.OrderedDict'>\n"
     ]
    }
   ],
   "source": [
    "print('type(weights)', type(weights))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, load the weights into the architecture we have created. After this the model is almost ready to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare prediction files\n",
    "\n",
    "To actually use the model, we need to download and prepare a set of recordings. The model was trained to make predictions on spectrograms made from 10 second-long recordings (from 'train-xenocanto-binary-bird-classifier' file), so we will have to split the recordings up and transform them into spectrograms.\n",
    "\n",
    "As example data, we have provided a 1 minute-long soundscape which contains Wood Thrush vocalizations. \n",
    "\n",
    "For reference check out this [link](https://github.com/kitzeslab/opensoundscape/blob/master/docs/predict.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example soundscape must be split up into soundscapes of the same size as the ones the model was trained on. In this case, the soundscapes should be 10s long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/e4e/Desktop/binary_bird_small_testset/original_data\n"
     ]
    }
   ],
   "source": [
    "data_directory = folder_path.joinpath(\"/home/e4e/Desktop/binary_bird_small_testset/original_data\")\n",
    "print(data_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to set a list of directories for each file. If you are on a mac then make sure to use the folloding bash command: find . -name \".DS_Store\" -delete\" in order to delete the .DS_Store file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_file_dir = []\n",
    "all_file_names = [] # get rid of the file extension\n",
    "for filename in os.listdir(data_directory): \n",
    "    all_file_names.append(os.path.splitext(filename)[0])\n",
    "    filepath = os.path.join(data_directory, filename)\n",
    "    all_file_dir.append(filepath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check the first couple of file directory strings and the filenames. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/e4e/Desktop/binary_bird_small_testset/original_data/_8MvhMlbwiE_40.000_clip_log.csv', '/home/e4e/Desktop/binary_bird_small_testset/original_data/XC22193 - Red-necked Woodpecker - Campephilus rubricollis_5s-10s.wav', '/home/e4e/Desktop/binary_bird_small_testset/original_data/XC22099 - White-bellied Tody-Tyrant - Hemitriccus griseipectus_5s-10s.wav', '/home/e4e/Desktop/binary_bird_small_testset/original_data/XC22035 - White-bellied Tody-Tyrant - Hemitriccus griseipectus_clip_log.csv', '/home/e4e/Desktop/binary_bird_small_testset/original_data/XC22211 - White-bellied Tody-Tyrant - Hemitriccus griseipectus_5s-10s.wav', '/home/e4e/Desktop/binary_bird_small_testset/original_data/_8MvhMlbwiE_40.000.wav', '/home/e4e/Desktop/binary_bird_small_testset/original_data/_6spzSHLoY0_30.000.wav', '/home/e4e/Desktop/binary_bird_small_testset/original_data/XC22211 - White-bellied Tody-Tyrant - Hemitriccus griseipectus_20s-25s.wav', '/home/e4e/Desktop/binary_bird_small_testset/original_data/XC22019 - Blue-crowned Trogon - Trogon curucui_15s-20s.wav', '/home/e4e/Desktop/binary_bird_small_testset/original_data/_1woPC5HWSg_30.000_5s-10s.wav']\n",
      "['_8MvhMlbwiE_40.000_clip_log', 'XC22193 - Red-necked Woodpecker - Campephilus rubricollis_5s-10s', 'XC22099 - White-bellied Tody-Tyrant - Hemitriccus griseipectus_5s-10s', 'XC22035 - White-bellied Tody-Tyrant - Hemitriccus griseipectus_clip_log', 'XC22211 - White-bellied Tody-Tyrant - Hemitriccus griseipectus_5s-10s', '_8MvhMlbwiE_40.000', '_6spzSHLoY0_30.000', 'XC22211 - White-bellied Tody-Tyrant - Hemitriccus griseipectus_20s-25s', 'XC22019 - Blue-crowned Trogon - Trogon curucui_15s-20s', '_1woPC5HWSg_30.000_5s-10s']\n"
     ]
    }
   ],
   "source": [
    "print(all_file_dir[:10])\n",
    "print(all_file_names[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, load the files as an `Audio` object and then split the recording in the audio object into segments of a specified length. Here, we split the clips into a 10s length, specify the directory they should be saved in (the `split_directory` just created) and specify the prefix of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_directory = \"/home/e4e/Desktop/binary_bird_small_testset/split_data_files\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NoBackendError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m~/acoustic-species-id-environments/Yoo-Jin/opensoundscape/opensoundscape-env/lib/python3.8/site-packages/librosa/core/audio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0msf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSoundFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msf_desc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m             \u001b[0msr_native\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msf_desc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamplerate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/acoustic-species-id-environments/Yoo-Jin/opensoundscape/opensoundscape-env/lib/python3.8/site-packages/soundfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, samplerate, channels, subtype, endian, format, closefd)\u001b[0m\n\u001b[1;32m    628\u001b[0m                                          format, subtype, endian)\n\u001b[0;32m--> 629\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode_int\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosefd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    630\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missuperset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'r+'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseekable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/acoustic-species-id-environments/Yoo-Jin/opensoundscape/opensoundscape-env/lib/python3.8/site-packages/soundfile.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(self, file, mode_int, closefd)\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid file: {0!r}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m         _error_check(_snd.sf_error(file_ptr),\n\u001b[0m\u001b[1;32m   1184\u001b[0m                      \"Error opening {0!r}: \".format(self.name))\n",
      "\u001b[0;32m~/acoustic-species-id-environments/Yoo-Jin/opensoundscape/opensoundscape-env/lib/python3.8/site-packages/soundfile.py\u001b[0m in \u001b[0;36m_error_check\u001b[0;34m(err, prefix)\u001b[0m\n\u001b[1;32m   1356\u001b[0m         \u001b[0merr_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_snd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msf_error_number\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1357\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0m_ffi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'replace'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error opening '/home/e4e/Desktop/binary_bird_small_testset/original_data/_8MvhMlbwiE_40.000_clip_log.csv': File contains data in an unknown format.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNoBackendError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-5eb286b9023a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_file_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mbase_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_file_dir\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     base_file.split_and_save(\n\u001b[1;32m      5\u001b[0m         \u001b[0mclip_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/acoustic-species-id-environments/Yoo-Jin/opensoundscape/opensoundscape-env/lib/python3.8/site-packages/opensoundscape/audio.py\u001b[0m in \u001b[0;36mfrom_file\u001b[0;34m(cls, path, sample_rate, max_duration, resample_type)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilterwarnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         samples, sr = librosa.load(\n\u001b[0m\u001b[1;32m     86\u001b[0m             \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresample_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmono\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         )\n",
      "\u001b[0;32m~/acoustic-species-id-environments/Yoo-Jin/opensoundscape/opensoundscape-env/lib/python3.8/site-packages/librosa/core/audio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'PySoundFile failed. Trying audioread instead.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr_native\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__audioread_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mduration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/acoustic-species-id-environments/Yoo-Jin/opensoundscape/opensoundscape-env/lib/python3.8/site-packages/librosa/core/audio.py\u001b[0m in \u001b[0;36m__audioread_load\u001b[0;34m(path, offset, duration, dtype)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0maudioread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudio_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0minput_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m         \u001b[0msr_native\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamplerate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0mn_channels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchannels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/acoustic-species-id-environments/Yoo-Jin/opensoundscape/opensoundscape-env/lib/python3.8/site-packages/audioread/__init__.py\u001b[0m in \u001b[0;36maudio_open\u001b[0;34m(path, backends)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;31m# All backends failed!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mNoBackendError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNoBackendError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(len(all_file_dir)): \n",
    "    base_file = Audio.from_file(all_file_dir[i])\n",
    "    \n",
    "    base_file.split_and_save(\n",
    "        clip_length=5,\n",
    "        destination = split_directory,\n",
    "        name = all_file_names[i]\n",
    "        dry = True # do not save the split wav files\n",
    "    ) \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that split_and_save outputs one .csv file for each file. So, let us compile all the .csv files into one in this for loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "os.chdir(split_directory)\n",
    "\n",
    "extension = 'csv'\n",
    "csv_filenames = [i for i in glob.glob('*.{}'.format(extension))]\n",
    "csv_filenames = [f for f in csv_filenames if not f.endswith('combined_csv.csv')]\n",
    "\n",
    "combined_csv = pd.concat([pd.read_csv(f, index_col='file')\n",
    "    for f in csv_filenames], ignore_index=False\n",
    ")\n",
    "\n",
    "print(combined_csv.columns)\n",
    "print(combined_csv['file'])\n",
    "\n",
    "#export to csv\n",
    "combined_csv.to_csv(\"combined_csv.csv\", index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(combined_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of this function, `split_files`, is a dataframe which identifies the filename of each split and its start and end times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_csv[:10]\n",
    "#print(combined_csv['file_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_csv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepend the \"file\" index with the directory in which the files are found so that the computer can refer to the files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(combined_csv.index[:10])\n",
    "print(combined_csv[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined_csv['file_path']\n",
    "print(type(combined_csv.index))\n",
    "print(combined_csv.index) #need to change into chronological order\n",
    "print(type(combined_csv))\n",
    "print(combined_csv)\n",
    "\n",
    "combined_csv['file_path'] = [split_directory.joinpath(csv) for csv in combined_csv.index]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Dataset\n",
    "\n",
    "Now that the data are split, we can create a \"dataset\" from them. \n",
    "\n",
    "To create a dataset requires that we create a dictionary that associates numeric labels with the class names: 1 is for predicting a bird's presence; 0 is for predicting a bird's absence. In more recent versions of the model, this label dictionary is packaged with the model download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {0:'bird-absent', 1:'bird-present'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a `SingleTargetAudioDataset`. This structure enables the data to be transformed into spectrograms when `test_dataset` is accessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = SingleTargetAudioDataset(\n",
    "    combined_csv,\n",
    "    filename_column = \"file_path\",\n",
    "    label_dict = label_dict,\n",
    "    save_dir=split_directory,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `test_dataset` is a list of dictionaries. Each element of the list contains a dictionary for one of the files to predict on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each dictionary in `test_dataset` has one or two keys. In all cases, the dictionary has a key `'X'` which refers to the spectrogram. If a dataset is created with true labels, the dictionary also has a `'y'` key which links to the true label. Because it is unknown which of these files contain Wood Thrush songs, no true labels were given when creating the dataset.\n",
    "\n",
    "The spectrogram itself is stored as a PyTorch tensor. For example, here is the tensor of the first spectrogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_tensor = test_dataset[0]['X']\n",
    "first_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view this spectrogram, use PyTorch's `transforms.ToPILImage()` function. This function returns a transformer. Call the transformer on the first tensor to display the spectrogram as an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = torchvision.transforms.ToPILImage()\n",
    "transformer(first_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use model on prediction files\n",
    "\n",
    "Finally, the model can be used for prediction. Use OpenSoundscape's `predict` function to call the model on the test dataset. The `label_dict` created above is used to make the classes interpretable; otherwise, the classes would just be numbered.\n",
    "\n",
    "Official documentation of train function from opensoundscape is [here](https://github.com/kitzeslab/opensoundscape/blob/master/opensoundscape/torch/predict.py)\n",
    "\n",
    "Notes: \n",
    "What is tensor?\n",
    "\"A PyTorch Tensor is basically the same as a numpy array: it does not know anything about deep learning or computational graphs or gradients, and is just a generic n-dimensional array to be used for arbitrary numeric computation.\" [source](https://pytorch.org/tutorials/beginner/examples_tensor/two_layer_net_tensor.html)\n",
    "\n",
    "Probability score: \n",
    "- Input a binary pytorch model\n",
    "- Uses enumerate(dataloader) for multi-process data loading\n",
    "- Uses softmax function which outputs numbers that represent probabilities where each number's value is between 0 and 1\n",
    "\n",
    "Questions: \n",
    "- What is the range of numbers here and what do they represent? \n",
    "- What is Dataloader?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df = predict(model, test_dataset, apply_softmax=True, label_dict=label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us save the results of the model being used for prediction as a csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df.to_csv('prediction_results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, this command \"cleans up\" by deleting all the downloaded files and results. Only run this if you are ready to remove the results of this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.rmtree(folder_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
