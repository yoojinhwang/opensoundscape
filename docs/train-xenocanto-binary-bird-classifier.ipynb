{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Bird Classifer from Using CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open Soundscape: http://opensoundscape.org/en/latest/train.html\n",
    "\n",
    "First, use the following packages to create a machine learning classifier. First, from OpenSoundscape import the following three functions (`run_command`, `binary_train_valid_split`, and `train`) and three classes (`Audio`, `Spectrogram`, and `SingleTargetAudioDataset`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opensoundscape.audio import Audio\n",
    "from opensoundscape.spectrogram import Spectrogram\n",
    "from opensoundscape.datasets import SingleTargetAudioDataset\n",
    "\n",
    "from opensoundscape.helpers import run_command\n",
    "from opensoundscape.data_selection import binary_train_valid_split\n",
    "from opensoundscape.torch.train import train\n",
    "\n",
    "# For working with dataframes, arrays, and plotting\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For importing data / audio-related python library\n",
    "import librosa\n",
    "import os\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the data into n-segment chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data\n",
    "Import all the audio files and resample to 44.1kHz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_n_second_multiples_and_filenames(dir_path, n): \n",
    "    \"\"\" \n",
    "    dir_path (path): path of a directory with desired files\n",
    "    n (int): split data into n-second clips\n",
    "    wav_data (n dimensional list)\n",
    "    \"\"\"\n",
    "    \n",
    "    wav_data = []\n",
    "    allfilenames = []\n",
    "    timestamp = []\n",
    "\n",
    "    for filename in os.listdir(dir_path):\n",
    "        filepath = os.path.join(dir_path, filename)\n",
    "        print(filepath)\n",
    "        \n",
    "        data, sr = librosa.load(filepath, sr=44100)\n",
    "        num_samples_in_n_seconds =  n  * sr \n",
    "        length_data = len(data)\n",
    "        length_in_seconds = length_data / sr \n",
    "        \n",
    "        if length_data < num_samples_in_n_seconds: \n",
    "            pass\n",
    "        \n",
    "        elif length_data > num_samples_in_n_seconds: \n",
    "            data = data[:-(length_data % num_samples_in_n_seconds)]\n",
    "            num_of_n_clips = int(np.floor(length_data / num_samples_in_n_seconds))\n",
    "            \n",
    "            for i in range(num_of_n_clips): \n",
    "                allfilenames.append(filename)\n",
    "                data_n_seconds = data[i*num_samples_in_n_seconds:(i+1)*num_samples_in_n_seconds]\n",
    "                timestamp.append([i*n,(i+1)*n])\n",
    "                wav_data.append(data_n_seconds)\n",
    "\n",
    "        else: #length_data == num_samples_in_n_seconds: \n",
    "            allfilenames.append(filename)\n",
    "            wav_data.append([data])\n",
    "            \n",
    "    return wav_data, allfilenames, timestamp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labelling Dataset\n",
    "\n",
    "In order to use a binary bird classifier with a CNN, we need to label the audio files we input to see if it has a bird or not. The dataset we will be using to train the CNN will ultimately have audio files from AudioSet and Xeno Canto data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = '/Volumes/Elements/Binary_Bird_Classifer_Small_Testset/data'\n",
    "wav_data, allfilenames, timestamp = find_n_second_multiples_and_filenames(dir_path, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are using xeno canto data (which is labelled data - bird and not bird), we need to label each of these files. In order to do so, we extract this information from the filename. Note that all the xeno canto data starts with 'XC'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_files(file_list): \n",
    "    bird_present_binary = []\n",
    "    bird_species_list = []\n",
    "    for f in file_list: \n",
    "        if f[:2] == 'XC':\n",
    "            bird_present_binary.append(1)\n",
    "            bird_species_list.append(os.path.splitext(f.split(\"-\",1)[1].strip())[0])\n",
    "        else:\n",
    "            bird_present_binary.append(0)\n",
    "            bird_species_list.append('N/A')\n",
    "    return bird_species_list, bird_present_binary\n",
    "            \n",
    "bird_species_list, bird_present_binary = label_files(allfilenames)\n",
    "\n",
    "assert(len(allfilenames)==len(bird_present_binary))\n",
    "assert(len(bird_species_list)==len(bird_present_binary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(bird_species_list, bird_present_binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put all this data into a Pandas dataframe\n",
    "There would be a pandas dataframe that would include filenames, the timestamp of the file, if a bird is present in the file, and the bird species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(allfilenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {'Filenames': allfilenames,\n",
    "        'Timestamp': timestamp,\n",
    "        'Bird present': bird_present_binary,\n",
    "        'Bird species': bird_species_list\n",
    "        }\n",
    "\n",
    "df = pd.DataFrame(labels, columns = ['Filenames', 'Timestamp', 'Bird present', 'Bird species'])\n",
    "label_dict = {0:'bird-absent', 1:'bird-present'}\n",
    "\n",
    "print(df)\n",
    "print(label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make allfilenames into a path \n",
    "\n",
    "# '/Volumes/Elements/Binary_Bird_Classifer_Small_Testset/[\\'5D0E8668.WAV\\', \\'5D0E8668.WAV\\', \\'5D0E8668.WAV\\', \\'5D0E8668.WAV\\', \\'5D0E8668.WAV\\', \\'5D0E8668.WAV\\', \\'5D0EA4E0.WAV\\', \\'5D0EA4E0.WAV\\', \\'5D0EA4E0.WAV\\', \\'5D0EA4E0.WAV\\', \\'5D0EA4E0.WAV\\', \\'5D0EA4E0.WAV\\', \\'5D0EA738.WAV\\', \\'5D0EA738.WAV\\', \\'5D0EA738.WAV\\', \\'5D0EA738.WAV\\', \\'5D0EA738.WAV\\', \\'5D0EA738.WAV\\', \\'5D0EA990.WAV\\', \\'5D0EA990.WAV\\', \\'5D0EA990.WAV\\', \\'5D0EA990.WAV\\', \\'5D0EA990.WAV\\', \\'5D0EA990.WAV\\', \\'5D0EABE8.WAV\\', \\'5D0EABE8.WAV\\', \\'5D0EABE8.WAV\\', \\'5D0EABE8.WAV\\', \\'5D0EABE8.WAV\\', \\'5D0EABE8.WAV\\', \\'5D0EAE40.WAV\\', \\'5D0EAE40.WAV\\', \\'5D0EAE40.WAV\\', \\'5D0EAE40.WAV\\', \\'5D0EAE40.WAV\\', \\'5D0EAE40.WAV\\', \\'5D0EB098.WAV\\', \\'5D0EB098.WAV\\', \\'5D0EB098.WAV\\', \\'5D0EB098.WAV\\', \\'5D0EB098.WAV\\', \\'5D0EB098.WAV\\', \\'5D0EB2F0.WAV\\', \\'5D0EB2F0.WAV\\', \\'5D0EB2F0.WAV\\', \\'5D0EB2F0.WAV\\', \\'5D0EB2F0.WAV\\', \\'5D0EB2F0.WAV\\', \\'5D0EC808.WAV\\', \\'5D0EC808.WAV\\', \\'5D0EC808.WAV\\', \\'5D0EC808.WAV\\', \\'5D0EC808.WAV\\', \\'5D0EC808.WAV\\', \\'5D0FC078.WAV\\', \\'5D0FC078.WAV\\', \\'5D0FC078.WAV\\', \\'5D0FC078.WAV\\', \\'5D0FC078.WAV\\', \\'5D0FC078.WAV\\', \\'XC22404 - Black Antbird - Cercomacroides serva.mp3\\', \\'XC22404 - Black Antbird - Cercomacroides serva.mp3\\', \\'XC22458 - White-eyed Antwren - Epinecrophylla leucophthalma.mp3\\', \\'XC22458 - White-eyed Antwren - Epinecrophylla leucophthalma.mp3\\', \\'XC23434 - White-winged Shrike-Tanager - Lanio versicolor.mp3\\', \\'XC23434 - White-winged Shrike-Tanager - Lanio versicolor.mp3\\', \"XC40313 - Goeldi\\'s Antbird - Akletos goeldii.mp3\", \"XC40313 - Goeldi\\'s Antbird - Akletos goeldii.mp3\", \"XC40313 - Goeldi\\'s Antbird - Akletos goeldii.mp3\", \"XC40313 - Goeldi\\'s Antbird - Akletos goeldii.mp3\", \"XC40313 - Goeldi\\'s Antbird - Akletos goeldii.mp3\", \"XC40313 - Goeldi\\'s Antbird - Akletos goeldii.mp3\"]'\n",
    "s = pd.Series(allfilenames)\n",
    "\n",
    "for i in range(len(allfilenames)): \n",
    "    df['Filenames'] = '/Volumes/Elements/Binary_Bird_Classifer_Small_Testset/' + allfilenames[i]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning: training\n",
    "\n",
    "Biologists are increasingly using acoustic recorders to study species of interest. Many bioacousticians want to determine the identity of the sounds they have recorded; a variety of manual and automated methods exist for this purpose. Automated methods can make it easier and faster to quickly predict which species or sounds are in one's recordings.\n",
    "\n",
    "Using a process called machine learning, bioacousticians can create (or \"train\") algorithms that can predict the identities of species vocalizing in acoustic recordings. These algorithms, called classifiers, typically do not identify sounds using the recording alone. Instead, they use image recognition techniques to identify sounds in spectrograms created from short segments of audio.\n",
    "\n",
    "This tutorial will guide you through the process of training a simple classifier for a single species. To download the tutorial as a Jupyter Notebook and run it on your own computer, click the \"Edit on GitHub\" button at the top right of the tutorial. You will have to [install OpenSoundscape](installation.html#installation) to use the tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the following machine learning-related modules. OpenSoundscape uses PyTorch to do machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn\n",
    "import torch.optim\n",
    "import torchvision.models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, use a few miscellaneous functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For interacting with paths on the filesystem\n",
    "import os.path\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# For handling output of the training function\n",
    "import io\n",
    "from contextlib import redirect_stdout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare audio data\n",
    "\n",
    "### Download labeled audio files\n",
    "\n",
    "Training a machine learning model requires some pre-labeled data. These data, in the form of audio recordings or spectrograms, are labeled with whether or not they contain the sound of the species of interest. These data can be obtained from online databases such as Xeno-Canto.org, or by labeling one's own ARU data using a program like Cornell's \"Raven\" sound analysis software.\n",
    "\n",
    "For ML pipelines, we need a training, validation, and test set. \n",
    "Outline: \n",
    "- Use audio augmented data of the actual test set for the training and validation set (split)\n",
    "- Then use the actual test set (collection of Xeno Canto data and Audioset) for the test set \n",
    "\n",
    "1. Training set: sample of data used to fit the model AKA the actual dataset that we use to train the model (audio augmented set of data)\n",
    "2. Validation: sample of data used to provide an unbiased evaluation of a model fit on the training dataset while tuning model hyperparameters\n",
    "3. Test set: sample of data used to provide an unbiased evaluation of a final model fit on the training dataset (usually unlabelled data)\n",
    "\n",
    "Sources: \n",
    "https://towardsdatascience.com/train-validation-and-test-sets-72cb40cba9e7 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audio Augmentation\n",
    "Reference for audio augmentation as seen here: https://github.com/UCSD-E4E/passive-acoustic-biodiversity/blob/master/Audio_Data_Augmentation/Data_Augmentation_Tutorial.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, valid_df = train_test_split(df, train_size=0.8, stratify=labels['Bird present'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = df.shape[0]\n",
    "num_present = sum(df['Bird present'] == 1)\n",
    "print(f\"Fraction of original dataframe with bird species present: {num_present/num_samples:.2f}\")\n",
    "\n",
    "num_train_samples = train_df.shape[0]\n",
    "num_train_present = sum(train_df['Bird present'] == 1)\n",
    "print(f\"Fraction of train samples with bird species present: {num_train_present/num_train_samples:.2f}\")\n",
    "    \n",
    "num_valid_samples = valid_df.shape[0]\n",
    "num_valid_present = sum(valid_df['Bird present'] == 1)\n",
    "print(f\"Fraction of train samples with bird species present: {num_valid_present/num_valid_samples:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format as `SingleTargetAudioDataset`s\n",
    "\n",
    "Turn these dataframes into \"Datasets\" using the `SingleTargetAudioDataset` class. Once they are set up in this class, they can be used by the training algorithm. Data augmentation could be applied in this step, but is not demonstrated here; for more information, see the [relevant API documentation](api.html#opensoundscape.datasets.SingleTargetAudioDataset).\n",
    "\n",
    "To use this class, specify the names of the relevant columns in the dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SingleTargetAudioDataset(\n",
    "    df=train_df, label_dict=label_dict, label_column='Bird present', filename_column='Filenames', add_noise=True)\n",
    "valid_dataset = SingleTargetAudioDataset(\n",
    "    df=valid_df, label_dict=label_dict, label_column='Bird present', filename_column='Filenames', add_noise=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the machine learning model\n",
    "Next, set up the architecture of the machine learning model and train it. \n",
    "\n",
    "### Set up model architecture\n",
    "The model architecture is a neural network. Neural networks are so-named for their loose similarity to neurons. Each **neuron** takes in a small amount of data, performs a transformation to the data, and passes it on with some weight to the next neuron. Neurons are usually organized in **layers**; each neuron in one layer can be connected to one or multiple neurons in the next layer. Complex structures can arise from this series of connections.\n",
    "\n",
    "The neural network used here is a combination of a feature extractor and a classifier. The **feature extractor** is a convolutional neural network (CNN). CNNs are a special class of neural network commonly used for image classification. They are able to interpret pixels that are near each other to identify shapes or textures in images, like lines, dots, and edges. During the training process, the CNN learns which shapes and textures are important for distinguishing between different classes.\n",
    "\n",
    "The specific CNN used here is `resnet18`, using the `pretrained=True` option. This means that the model loaded is a version that somebody has already trained on another image dataset called ImageNet, so it has a head start on understanding features commonly seen in images. Although spectrograms aren't the same type of images as the photographs used in ImageNet, using the pretrained model will allow the model to more quickly adapt to identifying spectrograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.resnet18(pretrained = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we refer to the whole neural network as a classifier, the part of the neural network that actually does the species classification is its `fc`, or \"fully connected,\" layers. This part of the neural network is called \"fully connected\" because it consists of several layers of neurons, where every neuron in each layer is connected to every other neuron in its adjacent layers.\n",
    "\n",
    "These layers come after the CNN layers, which have already interpreted an image's features. The `fc` layers then use those interpretations to classify the image. The number of output features of the CNN, therefore, is the number of input features of the `fc` layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc.in_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a `Linear` classifier for the `fc`. To set up the `Linear` classifier, identify the input and output size for this classifier. As described above, the `fc` takes in the outputs of the feature extractor, so `in_features = model.fc.in_features`. The model identifies one species, so it has to be able to output a \"present\" or \"absent\" classification. Thus, `out_features=2`. A multi-species model would use `out_features=number_of_species`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc = torch.nn.Linear(in_features = model.fc.in_features, out_features = 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "\n",
    "Next, create set up a directory in which to save results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path = Path('/Volumes/Elements/Binary_Bird_Classifer_Small_Testset/model_train_results')\n",
    "if not results_path.exists(): results_path.mkdir()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scikit-learn function may throw errors when calculating metrics; the following code will silence them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, run the model training with the following parameters:\n",
    "* `save_dir`: the directory in which to save results (which is created if it doesn't exist)\n",
    "* `model`: the model set up in the previous cell\n",
    "* `train_dataset`: the training dataset created using `SingleTargetAudioDataset`\n",
    "* `optimizer`: the optimizer to use for training the algorithm\n",
    "* `loss_fn`: the loss function used to assess the algorithm's performance during training\n",
    "* `epochs`: the number of times the model will run through the training data\n",
    "* `log_every`: how frequently to save performance data and save intermediate machine learning weights (`log_every=1` will save every epoch)\n",
    "\n",
    "The `train` function allows the user to control more parameters, but they are not demonstrated here. For more information, see the [train API](http://opensoundscape.org/en/latest/api.html#module-opensoundscape.torch.train)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_outputs = io.StringIO()\n",
    "print(train_outputs)\n",
    "\n",
    "with redirect_stdout(train_outputs):\n",
    "    train(\n",
    "        save_dir = results_path,\n",
    "        model = model,\n",
    "        train_dataset = train_dataset,\n",
    "        valid_dataset = valid_dataset,\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=1e-3),\n",
    "        loss_fn = torch.nn.CrossEntropyLoss(),\n",
    "        epochs=5,\n",
    "        log_every=1,\n",
    "        print_logging=True,\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model performance\n",
    "When training is complete, it is important to check the training results to see how well the model identifies sounds. This model was only trained on a limited amount of data, so the model is expected to not be usable--it is for demonstration purposes only.\n",
    "\n",
    "The outputs of the training function were saved to `train_outputs`. Check out the first 100 characters of this output.\n",
    "\n",
    "Notes on the statistics: \n",
    "- loss: In neural network, we want to minimize the error and the objective function for this is called a loss function. The value calculated by the loss function is referred to as \"loss\" \n",
    "- accuracy: Accuracy is the fraction of prediction our model got right aka the number of correct predictions / total number of predictions\n",
    "- precision: Answers the question: what proportion of positive identifications was actually correct? A model that produces no false positives (e.g. labels an audio file as a bird but does not contain any bird sounds) would have a precision of 1.0. Equation: TP / (TP + FP) \n",
    "- recall: Answers the question: what proportion of actual positives was identified correctly? A model that produces no false negatives has a recall of 1.0. So, in equation form: TP/ TP + FN\n",
    "- f1: A metric that relies on both precision and recall. \n",
    "\n",
    "Resources: \n",
    "- https://developers.google.com/machine-learning/crash-course/classification/precision-and-recall "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "source_text = train_outputs.getvalue()\n",
    "print(source_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These functions help to parse the log text. They simply extract the resulting \"metric\" in each epoch. Metrics include accuracy, precision, recall, and f1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_lines_containing(source_text, str_to_extract):\n",
    "    \"\"\"Case-sensitive search for lines containing str_to_extract\"\"\"\n",
    "    finished = False\n",
    "    lines = source_text.split('\\n')\n",
    "    extract_lines = [line for line in lines if str_to_extract in line]\n",
    "    return extract_lines\n",
    "\n",
    "def strip_log(log, sep=':     '):\n",
    "    return log.split(sep)[1]\n",
    "\n",
    "def get_metric_from_log(source_text, metric):\n",
    "    if 'precision' in metric or 'recall' in metric:\n",
    "        return [float(strip_log(line, sep=': ').strip('[]').split()[1]) for line in extract_all_lines_containing(source_text, metric)]\n",
    "    return [float(strip_log(line, sep=': ')) for line in extract_all_lines_containing(source_text, metric)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_metric_from_log(source_text, 'precision')\n",
    "# get_metric_from_log(source_text, 'accuracy')\n",
    "# get_metric_from_log(source_text, 'recall')\n",
    "# get_metric_from_log(source_text, 'f1 score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the validation accuracy each epoch. These results will look different every time the model is trained, as it is a stochastic process (randomly determined)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "metrics_to_plot = ['valid_accuracy', 'train_accuracy']\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "for metric in metrics_to_plot:\n",
    "    results = get_metric_from_log(source_text, metric)\n",
    "    ax.scatter(range(len(results)), results)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_title('model training results')\n",
    "ax.legend(metrics_to_plot)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, this command \"cleans up\" by deleting all the downloaded files and results. Only run this if you are ready to remove the results of this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "# Delete results\n",
    "shutil.rmtree(results_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
